{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Imports complete\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ Imports complete\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target stations: ['Central Park', 'JFK Airport', 'LaGuardia Airport']\n",
            "Data directory: /Users/drorjac/OpenMesh_pynncml/src/analysis/snow_analysis/../../data/noaa_asos/snow\n"
          ]
        }
      ],
      "source": [
        "# Station configurations\n",
        "STATION_CODES = {\n",
        "    'KNYC': 'Central Park',\n",
        "    'KJFK': 'JFK Airport',\n",
        "    'KLGA': 'LaGuardia Airport'\n",
        "}\n",
        "\n",
        "# Keywords to identify stations in NOAA data\n",
        "STATION_KEYWORDS = {\n",
        "    'KNYC': ['CENTRAL PARK', 'KNYC'],\n",
        "    'KJFK': ['JFK', 'JOHN F KENNEDY', 'KJFK'],\n",
        "    'KLGA': ['LAGUARDIA', 'LGA', 'KLGA']\n",
        "}\n",
        "\n",
        "# Data paths\n",
        "DATA_DIR = Path('../../data/noaa_asos/snow')\n",
        "OUTPUT_DIR = Path('../../data/noaa_asos/snow')\n",
        "\n",
        "print(f\"Target stations: {list(STATION_CODES.values())}\")\n",
        "print(f\"Data directory: {DATA_DIR.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Find and Process '41...' Files Only\n",
        "\n",
        "We only process the 417*.csv files (4177732.csv and 4177747.csv) for the selected stations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 '41...' files:\n",
            "  - 4177732.csv\n",
            "  - 4177747.csv\n"
          ]
        }
      ],
      "source": [
        "# Find only the '41...' CSV files (4177732.csv and 4177747.csv)\n",
        "data_files = list(DATA_DIR.glob('417*.csv'))\n",
        "print(f\"Found {len(data_files)} '41...' files:\")\n",
        "for f in sorted(data_files):\n",
        "    print(f\"  - {f.name}\")\n",
        "    \n",
        "if len(data_files) == 0:\n",
        "    print(\"âš  Warning: No '41...' files found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Read 4177732.csv: 93,166 rows, 28 columns\n",
            "  Stations: 130 unique\n",
            "    - US1NJUN0028: SPRINGFIELD TWP 0.7 NNE, NJ US\n",
            "    - US1NYSF0158: LINDENHURST 1.0 NE, NY US\n",
            "    - USW00014734: NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US\n",
            "\n",
            "âœ“ Read 4177747.csv: 1,062 rows, 44 columns\n",
            "  Stations: 1 unique\n",
            "    - USW00014732: LAGUARDIA AIRPORT, NY US\n",
            "\n",
            "âœ“ Read 2 '41...' files successfully\n"
          ]
        }
      ],
      "source": [
        "# Function to read '41...' NOAA files\n",
        "def read_41_file(filepath):\n",
        "    \"\"\"Read '41...' NOAA data file\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, low_memory=False)\n",
        "        print(f\"\\nâœ“ Read {filepath.name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
        "        \n",
        "        if 'STATION' in df.columns:\n",
        "            stations = df['STATION'].unique()\n",
        "            print(f\"  Stations: {len(stations)} unique\")\n",
        "            if 'NAME' in df.columns:\n",
        "                # Show first 3 stations\n",
        "                for st in stations[:3]:\n",
        "                    name = df[df['STATION'] == st]['NAME'].iloc[0] if len(df[df['STATION'] == st]) > 0 else 'Unknown'\n",
        "                    print(f\"    - {st}: {name}\")\n",
        "        else:\n",
        "            print(f\"  âš  Warning: No STATION column found\")\n",
        "        \n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error reading {filepath.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Read only the '41...' files\n",
        "dataframes_41 = []\n",
        "for filepath in data_files:\n",
        "    df = read_41_file(filepath)\n",
        "    if df is not None:\n",
        "        dataframes_41.append(df)\n",
        "\n",
        "print(f\"\\nâœ“ Read {len(dataframes_41)} '41...' files successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Process '41...' Files and Filter Selected Stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PROCESSING '41...' FILES\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Process '41...' files and filter for selected stations\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESSING '41...' FILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def identify_station(row):\n",
        "    \"\"\"Identify which of our target stations this row belongs to\"\"\"\n",
        "    station_name = str(row.get('NAME', '')).upper() if pd.notna(row.get('NAME')) else ''\n",
        "    station_id = str(row.get('STATION', '')).upper() if pd.notna(row.get('STATION')) else ''\n",
        "    \n",
        "    for code, keywords in STATION_KEYWORDS.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword.upper() in station_name or keyword.upper() in station_id:\n",
        "                return code\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Filter and Process Selected Stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Combined '41...' files: 94,228 rows\n",
            "  Converted DATE column to datetime\n",
            "  âœ“ Filtered to target stations: 4,158 rows\n",
            "\n",
            "  Stations found:\n",
            "    KLGA (LaGuardia Airport): 2,094 rows\n",
            "    KJFK (JFK Airport): 1,032 rows\n",
            "    KNYC (Central Park): 1,032 rows\n",
            "\n",
            "  Selected columns: ['DATE', 'station_code', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TAVG', 'AWND', 'WSF2', 'WSF5', 'WDF2', 'WDF5', 'STATION', 'NAME']\n",
            "âœ“ Processing complete\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Process '41...' files and filter for selected stations\n",
        "# ============================================================================\n",
        "\n",
        "if len(dataframes_41) > 0:\n",
        "    # Combine all '41...' files\n",
        "    df_combined = pd.concat(dataframes_41, ignore_index=True)\n",
        "    print(f\"\\nâœ“ Combined '41...' files: {len(df_combined):,} rows\")\n",
        "    \n",
        "    # Convert DATE to datetime\n",
        "    if 'DATE' in df_combined.columns:\n",
        "        df_combined['DATE'] = pd.to_datetime(df_combined['DATE'], errors='coerce')\n",
        "        print(f\"  Converted DATE column to datetime\")\n",
        "    \n",
        "    # Identify target stations\n",
        "    df_combined['station_code'] = df_combined.apply(identify_station, axis=1)\n",
        "    df_filtered = df_combined[df_combined['station_code'].notna()].copy()\n",
        "    \n",
        "    if len(df_filtered) > 0:\n",
        "        print(f\"  âœ“ Filtered to target stations: {len(df_filtered):,} rows\")\n",
        "        \n",
        "        # Show what stations were found\n",
        "        station_counts = df_filtered['station_code'].value_counts()\n",
        "        print(f\"\\n  Stations found:\")\n",
        "        for code, count in station_counts.items():\n",
        "            station_name = STATION_CODES.get(code, code)\n",
        "            print(f\"    {code} ({station_name}): {count:,} rows\")\n",
        "        \n",
        "        # Select relevant columns\n",
        "        relevant_cols = ['DATE', 'station_code', 'PRCP', 'SNOW', 'SNWD', \n",
        "                        'TMAX', 'TMIN', 'TAVG', 'AWND', 'WSF2', 'WSF5', \n",
        "                        'WDF2', 'WDF5', 'STATION', 'NAME']\n",
        "        \n",
        "        available_cols = [col for col in relevant_cols if col in df_filtered.columns]\n",
        "        df_snow = df_filtered[available_cols].copy()\n",
        "        \n",
        "        # Convert numeric columns\n",
        "        numeric_cols = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TAVG', \n",
        "                       'AWND', 'WSF2', 'WSF5', 'WDF2', 'WDF5']\n",
        "        for col in numeric_cols:\n",
        "            if col in df_snow.columns:\n",
        "                df_snow[col] = pd.to_numeric(df_snow[col], errors='coerce')\n",
        "        \n",
        "        print(f\"\\n  Selected columns: {list(df_snow.columns)}\")\n",
        "        print(f\"âœ“ Processing complete\")\n",
        "    else:\n",
        "        df_snow = None\n",
        "        print(\"  âš  No target stations found in '41...' files\")\n",
        "else:\n",
        "    df_snow = None\n",
        "    print(\"  âš  No '41...' files to process\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Unit Conversion and Rain Date Accumulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1. Unit Conversion\n",
        "\n",
        "Convert precipitation and snow from inches to mm for consistent units\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "UNIT CONVERSION: Converting from inches to mm\n",
            "============================================================\n",
            "\n",
            "Converting columns from inches to mm:\n",
            "  PRCP:\n",
            "    Original (inches): max=8.05, mean=0.128\n",
            "    Converted (mm): max=204.47, mean=3.240\n",
            "  SNOW:\n",
            "    Original (inches): max=6.20, mean=0.026\n",
            "    Converted (mm): max=157.48, mean=0.656\n",
            "  SNWD:\n",
            "    Original (inches): max=5.90, mean=0.039\n",
            "    Converted (mm): max=149.86, mean=0.990\n",
            "\n",
            "âœ“ Unit conversion complete - all values now in mm\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# UNIT CONVERSION: Convert from inches to mm\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"UNIT CONVERSION: Converting from inches to mm\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if df_snow is not None:\n",
        "    # Convert precipitation and snow columns from inches to mm\n",
        "    # Conversion factor: 1 inch = 25.4 mm\n",
        "    \n",
        "    # Columns to convert (precipitation and snow-related)\n",
        "    columns_to_convert = ['PRCP', 'SNOW', 'SNWD']\n",
        "    \n",
        "    print(f\"\\nConverting columns from inches to mm:\")\n",
        "    for col in columns_to_convert:\n",
        "        if col in df_snow.columns:\n",
        "            # Store original stats\n",
        "            non_null_count = df_snow[col].notna().sum()\n",
        "            if non_null_count > 0:\n",
        "                original_max = df_snow[col].max()\n",
        "                original_mean = df_snow[col].mean()\n",
        "                \n",
        "                # Convert to mm\n",
        "                df_snow[col] = df_snow[col] * 25.4\n",
        "                \n",
        "                # Show conversion stats\n",
        "                print(f\"  {col}:\")\n",
        "                print(f\"    Original (inches): max={original_max:.2f}, mean={original_mean:.3f}\")\n",
        "                print(f\"    Converted (mm): max={df_snow[col].max():.2f}, mean={df_snow[col].mean():.3f}\")\n",
        "            else:\n",
        "                print(f\"  {col}: No data to convert\")\n",
        "    \n",
        "    print(f\"\\nâœ“ Unit conversion complete - all values now in mm\")\n",
        "else:\n",
        "    print(\"âš  df_snow not available for conversion\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. Rain Date Accumulation\n",
        "\n",
        "Add columns to track rain dates, cumulative precipitation, and rain events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RAIN DATE ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Rain Statistics by Station:\n",
            "Station  Rain Days    Total PRCP (mm)    Max PRCP (mm)  \n",
            "------------------------------------------------------------\n",
            "KJFK     344          2983.23            204.47         \n",
            "KLGA     690          6906.01            104.14         \n",
            "KNYC     343          3490.47            139.19         \n",
            "\n",
            "Date Range:\n",
            "  Min date: 2023-01-01\n",
            "  Max date: 2025-11-27\n",
            "  Total days: 1062\n",
            "\n",
            "âœ“ Rain date accumulation complete\n",
            "  Columns added: is_rain_day, PRCP_cumulative, rain_event\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RAIN DATE ACCUMULATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RAIN DATE ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if df_snow is not None:\n",
        "    # Sort by station and date for proper accumulation\n",
        "    df_snow = df_snow.sort_values(['station_code', 'DATE']).reset_index(drop=True)\n",
        "    \n",
        "    # Identify rain days (PRCP > 0)\n",
        "    df_snow['is_rain_day'] = (df_snow['PRCP'] > 0) & (df_snow['PRCP'].notna())\n",
        "    \n",
        "    # Calculate cumulative precipitation per station (resets per station)\n",
        "    df_snow['PRCP_cumulative'] = df_snow.groupby('station_code')['PRCP'].cumsum()\n",
        "    \n",
        "    # Track rain event number (each consecutive rain day gets same event number)\n",
        "    df_snow['rain_event'] = 0\n",
        "    for station in df_snow['station_code'].unique():\n",
        "        station_mask = df_snow['station_code'] == station\n",
        "        station_data = df_snow.loc[station_mask, 'is_rain_day'].copy()\n",
        "        \n",
        "        # Create rain events: consecutive True values are same event\n",
        "        events = (station_data != station_data.shift()).cumsum()\n",
        "        df_snow.loc[station_mask, 'rain_event'] = events.where(station_data, 0)\n",
        "    \n",
        "    # Count rain days per station\n",
        "    rain_stats = df_snow.groupby('station_code').agg({\n",
        "        'is_rain_day': 'sum',\n",
        "        'PRCP': ['sum', 'mean', 'max'],\n",
        "        'DATE': ['min', 'max']\n",
        "    }).round(2)\n",
        "    \n",
        "    print(f\"\\nRain Statistics by Station:\")\n",
        "    print(f\"{'Station':<8} {'Rain Days':<12} {'Total PRCP (mm)':<18} {'Max PRCP (mm)':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    for station in sorted(df_snow['station_code'].unique()):\n",
        "        station_data = df_snow[df_snow['station_code'] == station]\n",
        "        rain_days = station_data['is_rain_day'].sum()\n",
        "        total_prcp = station_data['PRCP'].sum()\n",
        "        max_prcp = station_data['PRCP'].max()\n",
        "        station_name = STATION_CODES.get(station, station)\n",
        "        print(f\"{station:<8} {rain_days:<12} {total_prcp:<18.2f} {max_prcp:<15.2f}\")\n",
        "    \n",
        "    # Add date range info\n",
        "    print(f\"\\nDate Range:\")\n",
        "    print(f\"  Min date: {df_snow['DATE'].min().date()}\")\n",
        "    print(f\"  Max date: {df_snow['DATE'].max().date()}\")\n",
        "    print(f\"  Total days: {(df_snow['DATE'].max() - df_snow['DATE'].min()).days + 1}\")\n",
        "    \n",
        "    print(f\"\\nâœ“ Rain date accumulation complete\")\n",
        "    print(f\"  Columns added: is_rain_day, PRCP_cumulative, rain_event\")\n",
        "else:\n",
        "    print(\"âš  df_snow not available for rain date accumulation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL PROCESSING - CLEAN DATAFRAME\n",
            "============================================================\n",
            "  Removed 1032 duplicate rows\n",
            "\n",
            "âœ“ df_snow ready:\n",
            "  Rows: 3,126\n",
            "  Columns: ['DATE', 'station_code', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TAVG', 'AWND', 'WSF2', 'WSF5', 'WDF2', 'WDF5', 'STATION', 'NAME', 'is_rain_day', 'PRCP_cumulative', 'rain_event']\n",
            "  Date range: 2023-01-01 to 2025-11-27\n",
            "  Stations: KJFK, KLGA, KNYC\n",
            "\n",
            "ðŸ“Š Data Summary:\n",
            "   KJFK (JFK Airport): 1,032 days, 344 rain days\n",
            "   KLGA (LaGuardia Airport): 1,062 days, 352 rain days\n",
            "   KNYC (Central Park): 1,032 days, 343 rain days\n",
            "\n",
            "âœ“ Final dataframe ready for analysis and saving\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FINAL PROCESSING - Clean and finalize df_snow\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL PROCESSING - CLEAN DATAFRAME\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if df_snow is not None:\n",
        "    # Ensure DATE is datetime and sorted\n",
        "    df_snow['DATE'] = pd.to_datetime(df_snow['DATE'], errors='coerce')\n",
        "    df_snow = df_snow.sort_values(['station_code', 'DATE']).reset_index(drop=True)\n",
        "    \n",
        "    # Remove duplicate rows if any\n",
        "    initial_rows = len(df_snow)\n",
        "    df_snow = df_snow.drop_duplicates(subset=['DATE', 'station_code'], keep='first').reset_index(drop=True)\n",
        "    if len(df_snow) < initial_rows:\n",
        "        print(f\"  Removed {initial_rows - len(df_snow)} duplicate rows\")\n",
        "    \n",
        "    print(f\"\\nâœ“ df_snow ready:\")\n",
        "    print(f\"  Rows: {len(df_snow):,}\")\n",
        "    print(f\"  Columns: {list(df_snow.columns)}\")\n",
        "    print(f\"  Date range: {df_snow['DATE'].min().date()} to {df_snow['DATE'].max().date()}\")\n",
        "    print(f\"  Stations: {', '.join(sorted(df_snow['station_code'].dropna().unique()))}\")\n",
        "    \n",
        "    # Show summary\n",
        "    print(f\"\\nðŸ“Š Data Summary:\")\n",
        "    for station in sorted(df_snow['station_code'].dropna().unique()):\n",
        "        station_data = df_snow[df_snow['station_code'] == station]\n",
        "        rain_days = station_data['is_rain_day'].sum() if 'is_rain_day' in station_data.columns else 0\n",
        "        print(f\"   {station} ({STATION_CODES.get(station, station)}): \"\n",
        "              f\"{len(station_data):,} days, {rain_days} rain days\")\n",
        "    \n",
        "    print(f\"\\nâœ“ Final dataframe ready for analysis and saving\")\n",
        "else:\n",
        "    print(\"\\nâš  df_snow: Not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Preview Data\n",
        "\n",
        "Preview the processed dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "df_snow PREVIEW\n",
            "============================================================\n",
            "\n",
            "First 15 rows:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         DATE station_code   PRCP  SNOW  SNWD  TMAX  TMIN  TAVG   AWND  WSF2  \\\n",
            "0  2023-01-31         KJFK  0.762  0.00   0.0  45.0  31.0  39.0  10.29  21.0   \n",
            "1  2023-02-01         KJFK  0.508  5.08   0.0  38.0  26.0  32.0  10.51  16.1   \n",
            "2  2023-02-02         KJFK  0.000  0.00   0.0  39.0  24.0  32.0  10.29  23.0   \n",
            "3  2023-02-03         KJFK  0.000  0.00   0.0  35.0  11.0  29.0  25.72  42.9   \n",
            "4  2023-02-04         KJFK  0.000  0.00   0.0  27.0   4.0  12.0  17.00  36.9   \n",
            "5  2023-02-05         KJFK  0.000  0.00   0.0  44.0  27.0  33.0  12.30  25.1   \n",
            "6  2023-02-06         KJFK  0.000  0.00   0.0  53.0  32.0  42.0  12.97  31.1   \n",
            "7  2023-02-07         KJFK  0.508  0.00   0.0  41.0  28.0  35.0  10.07  19.9   \n",
            "8  2023-02-08         KJFK  0.254  0.00   0.0  54.0  34.0  43.0  11.41  25.9   \n",
            "9  2023-02-09         KJFK  0.000  0.00   0.0  50.0  30.0  42.0   8.05  23.0   \n",
            "10 2023-02-10         KJFK  0.000  0.00   0.0  60.0  41.0  52.0  15.66  29.1   \n",
            "11 2023-02-11         KJFK  0.000  0.00   0.0  47.0  35.0  43.0  13.65  28.0   \n",
            "12 2023-02-12         KJFK  1.016  0.00   0.0  45.0  31.0  39.0   6.26  19.9   \n",
            "13 2023-02-13         KJFK  0.000  0.00   0.0  56.0  38.0  44.0  13.87  21.9   \n",
            "14 2023-02-14         KJFK  0.000  0.00   0.0  53.0  41.0  47.0  14.32  29.1   \n",
            "\n",
            "    WSF5   WDF2   WDF5      STATION                              NAME  \\\n",
            "0   25.9  350.0  350.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "1   19.0   30.0  320.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "2   28.0  230.0  220.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "3   51.0  310.0  310.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "4   49.0  320.0  320.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "5   32.0  210.0  210.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "6   40.9  330.0  360.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "7   23.9  180.0  180.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "8   33.1  330.0  310.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "9   28.0  170.0  180.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "10  36.9  290.0  270.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "11  38.9  310.0  310.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "12  23.9   20.0   10.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "13  25.1  250.0  250.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "14  35.1  310.0  320.0  USW00094789  JFK INTERNATIONAL AIRPORT, NY US   \n",
            "\n",
            "    is_rain_day  PRCP_cumulative  rain_event  \n",
            "0          True            0.762           1  \n",
            "1          True            1.270           1  \n",
            "2         False            1.270           0  \n",
            "3         False            1.270           0  \n",
            "4         False            1.270           0  \n",
            "5         False            1.270           0  \n",
            "6         False            1.270           0  \n",
            "7          True            1.778           3  \n",
            "8          True            2.032           3  \n",
            "9         False            2.032           0  \n",
            "10        False            2.032           0  \n",
            "11        False            2.032           0  \n",
            "12         True            3.048           5  \n",
            "13        False            3.048           0  \n",
            "14        False            3.048           0  \n",
            "\n",
            "Dataframe info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3126 entries, 0 to 3125\n",
            "Data columns (total 18 columns):\n",
            " #   Column           Non-Null Count  Dtype         \n",
            "---  ------           --------------  -----         \n",
            " 0   DATE             3126 non-null   datetime64[ns]\n",
            " 1   station_code     3126 non-null   object        \n",
            " 2   PRCP             3097 non-null   float64       \n",
            " 3   SNOW             3096 non-null   float64       \n",
            " 4   SNWD             3088 non-null   float64       \n",
            " 5   TMAX             3096 non-null   float64       \n",
            " 6   TMIN             3097 non-null   float64       \n",
            " 7   TAVG             1910 non-null   float64       \n",
            " 8   AWND             2811 non-null   float64       \n",
            " 9   WSF2             2817 non-null   float64       \n",
            " 10  WSF5             2815 non-null   float64       \n",
            " 11  WDF2             2817 non-null   float64       \n",
            " 12  WDF5             2815 non-null   float64       \n",
            " 13  STATION          3126 non-null   object        \n",
            " 14  NAME             3126 non-null   object        \n",
            " 15  is_rain_day      3126 non-null   bool          \n",
            " 16  PRCP_cumulative  3097 non-null   float64       \n",
            " 17  rain_event       3126 non-null   int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(12), int64(1), object(3)\n",
            "memory usage: 418.4+ KB\n",
            "\n",
            "Columns: DATE, station_code, PRCP, SNOW, SNWD, TMAX, TMIN, TAVG, AWND, WSF2, WSF5, WDF2, WDF5, STATION, NAME, is_rain_day, PRCP_cumulative, rain_event\n"
          ]
        }
      ],
      "source": [
        "# Preview df_snow\n",
        "if 'df_snow' in locals() and df_snow is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"df_snow PREVIEW\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nFirst 15 rows:\")\n",
        "    print(df_snow.head(15))\n",
        "    print(f\"\\nDataframe info:\")\n",
        "    df_snow.info()\n",
        "    print(f\"\\nColumns: {', '.join(df_snow.columns)}\")\n",
        "else:\n",
        "    print(\"âš  df_snow not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SUMMARY STATISTICS\n",
            "============================================================\n",
            "\n",
            "df_snow - Summary Statistics:\n",
            "              PRCP         SNOW         SNWD         TMAX         TMIN  \\\n",
            "count  3097.000000  3096.000000  3088.000000  3096.000000  3097.000000   \n",
            "mean      3.224168     0.640743     1.033931    65.284561    51.155957   \n",
            "std       9.711931     6.254449     7.093941    16.608952    15.175988   \n",
            "min       0.000000     0.000000     0.000000    19.000000     3.000000   \n",
            "25%       0.000000     0.000000     0.000000    52.000000    39.000000   \n",
            "50%       0.000000     0.000000     0.000000    66.000000    51.000000   \n",
            "75%       1.270000     0.000000     0.000000    80.000000    64.000000   \n",
            "max     204.470000   157.480000   149.860000   102.000000    81.000000   \n",
            "\n",
            "              TAVG         AWND         WSF2         WSF5         WDF2  \\\n",
            "count  1910.000000  2811.000000  2817.000000  2815.000000  2817.000000   \n",
            "mean     57.492670     8.830338    18.820767    26.798259   204.345048   \n",
            "std      15.770841     4.398227     6.738706     8.695361   101.207231   \n",
            "min      12.000000     0.670000     6.000000     8.900000    10.000000   \n",
            "25%      44.000000     5.590000    14.100000    21.000000   120.000000   \n",
            "50%      58.000000     8.500000    17.000000    25.100000   220.000000   \n",
            "75%      71.000000    11.410000    23.000000    31.100000   300.000000   \n",
            "max      90.000000    26.620000    45.000000    68.000000   360.000000   \n",
            "\n",
            "              WDF5  PRCP_cumulative   rain_event  \n",
            "count  2815.000000      3097.000000  3126.000000  \n",
            "mean    202.034813      2448.426890    62.970889  \n",
            "std     101.543256      1760.817521   108.095058  \n",
            "min      10.000000         0.508000     0.000000  \n",
            "25%     120.000000      1106.932000     0.000000  \n",
            "50%     210.000000      2250.694000     0.000000  \n",
            "75%     300.000000      3211.830000    97.000000  \n",
            "max     360.000000      6906.006000   383.000000  \n",
            "\n",
            "Rain Day Statistics:\n",
            "  Total rain days: 1,039\n",
            "  Percentage of days with rain: 33.2%\n",
            "  Average rain on rain days: 9.61 mm\n",
            "  Max daily rainfall: 204.47 mm\n"
          ]
        }
      ],
      "source": [
        "# Summary statistics for df_snow\n",
        "if 'df_snow' in locals() and df_snow is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Select numeric columns (exclude metadata)\n",
        "    numeric_cols = df_snow.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in ['LATITUDE', 'LONGITUDE', 'ELEVATION']]\n",
        "    \n",
        "    if len(numeric_cols) > 0:\n",
        "        print(\"\\ndf_snow - Summary Statistics:\")\n",
        "        print(df_snow[numeric_cols].describe())\n",
        "    \n",
        "    # Show rain day statistics\n",
        "    if 'is_rain_day' in df_snow.columns:\n",
        "        print(\"\\nRain Day Statistics:\")\n",
        "        print(f\"  Total rain days: {df_snow['is_rain_day'].sum():,}\")\n",
        "        print(f\"  Percentage of days with rain: {df_snow['is_rain_day'].mean()*100:.1f}%\")\n",
        "        \n",
        "        if 'PRCP' in df_snow.columns:\n",
        "            rain_only = df_snow[df_snow['is_rain_day']]\n",
        "            if len(rain_only) > 0:\n",
        "                print(f\"  Average rain on rain days: {rain_only['PRCP'].mean():.2f} mm\")\n",
        "                print(f\"  Max daily rainfall: {rain_only['PRCP'].max():.2f} mm\")\n",
        "else:\n",
        "    print(\"âš  df_snow not available for statistics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "âœ“ DATA SAVED\n",
            "============================================================\n",
            "\n",
            "âœ“ Saved df_snow to: snow_data_processed.csv\n",
            "  Total records: 3,126\n",
            "  Stations: KJFK, KLGA, KNYC\n",
            "  Columns: DATE, station_code, PRCP, SNOW, SNWD, TMAX, TMIN, TAVG, AWND, WSF2, WSF5, WDF2, WDF5, STATION, NAME, is_rain_day, PRCP_cumulative, rain_event\n",
            "  Date range: 2023-01-01 to 2025-11-27\n",
            "\n",
            "âœ“ DATA READY FOR ANALYSIS\n",
            "\n",
            "Available dataframe: df_snow\n",
            "  Source: '41...' files (4177732.csv, 4177747.csv)\n",
            "  Contains: PRCP, SNOW, SNWD, temperature data, and rain date accumulation\n"
          ]
        }
      ],
      "source": [
        "# Save df_snow\n",
        "if 'df_snow' in locals() and df_snow is not None:\n",
        "    output_file = OUTPUT_DIR / 'snow_data_processed.csv'\n",
        "    df_snow.to_csv(output_file, index=False)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ“ DATA SAVED\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nâœ“ Saved df_snow to: {output_file.name}\")\n",
        "    print(f\"  Total records: {len(df_snow):,}\")\n",
        "    print(f\"  Stations: {', '.join(sorted(df_snow['station_code'].dropna().unique()))}\")\n",
        "    print(f\"  Columns: {', '.join(df_snow.columns)}\")\n",
        "    print(f\"  Date range: {df_snow['DATE'].min().date()} to {df_snow['DATE'].max().date()}\")\n",
        "    print(f\"\\nâœ“ DATA READY FOR ANALYSIS\")\n",
        "    print(f\"\\nAvailable dataframe: df_snow\")\n",
        "    print(f\"  Source: '41...' files (4177732.csv, 4177747.csv)\")\n",
        "    print(f\"  Contains: PRCP, SNOW, SNWD, temperature data, and rain date accumulation\")\n",
        "else:\n",
        "    print(\"\\nâš  df_snow not available to save\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Data Ready for Analysis\n",
        "\n",
        "You now have two separate dataframes:\n",
        "\n",
        "**`df_noaa_a`** - Processed files:\n",
        "- Source: pcpn_noaa.csv, snow_noaa.csv, snwd_noaa.csv\n",
        "- Columns: DATE, station_code, pcpn_noaa, snow_noaa, snwd_noaa\n",
        "- Format: Already processed and aggregated by station\n",
        "\n",
        "**`df_noaa_b`** - Raw NOAA files:\n",
        "- Source: 417*.csv files\n",
        "- Columns: DATE, station_code, PRCP, SNOW, SNWD, TMAX, TMIN, TAVG, etc.\n",
        "- Format: Raw NOAA data filtered for target stations\n",
        "\n",
        "Both dataframes can be analyzed separately or compared (e.g., pcpn_noaa vs PRCP).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Analysis\n",
        "\n",
        "Start your analysis here using `df_noaa_a` and `df_noaa_b`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis 1: Basic Statistics by Station\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_noaa_a not available\n"
          ]
        }
      ],
      "source": [
        "# Basic statistics by station for df_noaa_a\n",
        "if 'df_noaa_a' in locals() and df_noaa_a is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"df_noaa_a - Statistics by Station\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for station in sorted(df_noaa_a['station_code'].unique()):\n",
        "        station_data = df_noaa_a[df_noaa_a['station_code'] == station]\n",
        "        print(f\"\\n{station} ({STATION_CODES.get(station, station)}):\")\n",
        "        \n",
        "        if 'pcpn_noaa' in station_data.columns:\n",
        "            pcpn = station_data['pcpn_noaa'].dropna()\n",
        "            print(f\"  Precipitation (pcpn_noaa):\")\n",
        "            print(f\"    Total: {pcpn.sum():.2f} mm\")\n",
        "            print(f\"    Mean: {pcpn.mean():.2f} mm/day\")\n",
        "            print(f\"    Max: {pcpn.max():.2f} mm/day\")\n",
        "            print(f\"    Days with rain: {(pcpn > 0).sum()}\")\n",
        "        \n",
        "        if 'snow_noaa' in station_data.columns:\n",
        "            snow = station_data['snow_noaa'].dropna()\n",
        "            print(f\"  Snow (snow_noaa):\")\n",
        "            print(f\"    Total: {snow.sum():.2f} mm\")\n",
        "            print(f\"    Mean: {snow.mean():.2f} mm/day\")\n",
        "            print(f\"    Max: {snow.max():.2f} mm/day\")\n",
        "            print(f\"    Days with snow: {(snow > 0).sum()}\")\n",
        "else:\n",
        "    print(\"df_noaa_a not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_noaa_b not available\n"
          ]
        }
      ],
      "source": [
        "# Basic statistics by station for df_noaa_b\n",
        "if 'df_noaa_b' in locals() and df_noaa_b is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"df_noaa_b - Statistics by Station\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for station in sorted(df_noaa_b['station_code'].dropna().unique()):\n",
        "        station_data = df_noaa_b[df_noaa_b['station_code'] == station]\n",
        "        print(f\"\\n{station} ({STATION_CODES.get(station, station)}):\")\n",
        "        \n",
        "        if 'PRCP' in station_data.columns:\n",
        "            prcp = station_data['PRCP'].dropna()\n",
        "            print(f\"  Precipitation (PRCP):\")\n",
        "            print(f\"    Total: {prcp.sum():.2f} mm\")\n",
        "            print(f\"    Mean: {prcp.mean():.2f} mm/day\")\n",
        "            print(f\"    Max: {prcp.max():.2f} mm/day\")\n",
        "            print(f\"    Days with rain: {(prcp > 0).sum()}\")\n",
        "        \n",
        "        if 'SNOW' in station_data.columns:\n",
        "            snow = station_data['SNOW'].dropna()\n",
        "            print(f\"  Snow (SNOW):\")\n",
        "            print(f\"    Total: {snow.sum():.2f} mm\")\n",
        "            print(f\"    Mean: {snow.mean():.2f} mm/day\")\n",
        "            print(f\"    Max: {snow.max():.2f} mm/day\")\n",
        "            print(f\"    Days with snow: {(snow > 0).sum()}\")\n",
        "else:\n",
        "    print(\"df_noaa_b not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis 2: Compare Precipitation Data Sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Both dataframes not available for comparison\n"
          ]
        }
      ],
      "source": [
        "# Compare precipitation from df_noaa_a (pcpn_noaa) vs df_noaa_b (PRCP)\n",
        "# This creates a comparison dataframe when dates overlap\n",
        "\n",
        "if 'df_noaa_a' in locals() and 'df_noaa_b' in locals() and df_noaa_a is not None and df_noaa_b is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"PRECIPITATION COMPARISON: pcpn_noaa (df_noaa_a) vs PRCP (df_noaa_b)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Merge on DATE and station_code to compare\n",
        "    comparison = pd.merge(\n",
        "        df_noaa_a[['DATE', 'station_code', 'pcpn_noaa']],\n",
        "        df_noaa_b[['DATE', 'station_code', 'PRCP']],\n",
        "        on=['DATE', 'station_code'],\n",
        "        how='inner'  # Only dates that exist in both\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nOverlapping dates: {len(comparison):,} days\")\n",
        "    print(f\"Date range: {comparison['DATE'].min().date()} to {comparison['DATE'].max().date()}\")\n",
        "    \n",
        "    # Calculate differences\n",
        "    comparison['diff'] = comparison['pcpn_noaa'] - comparison['PRCP']\n",
        "    comparison['diff_pct'] = (comparison['diff'] / comparison['PRCP'] * 100).replace([np.inf, -np.inf], np.nan)\n",
        "    \n",
        "    print(f\"\\nComparison Statistics:\")\n",
        "    print(f\"  Mean difference: {comparison['diff'].mean():.3f} mm\")\n",
        "    print(f\"  RMSE: {np.sqrt((comparison['diff']**2).mean()):.3f} mm\")\n",
        "    print(f\"  Correlation: {comparison['pcpn_noaa'].corr(comparison['PRCP']):.3f}\")\n",
        "    \n",
        "    print(f\"\\nBy Station:\")\n",
        "    for station in sorted(comparison['station_code'].unique()):\n",
        "        station_comp = comparison[comparison['station_code'] == station]\n",
        "        print(f\"\\n  {station}:\")\n",
        "        print(f\"    Correlation: {station_comp['pcpn_noaa'].corr(station_comp['PRCP']):.3f}\")\n",
        "        print(f\"    Mean difference: {station_comp['diff'].mean():.3f} mm\")\n",
        "        print(f\"    RMSE: {np.sqrt((station_comp['diff']**2).mean()):.3f} mm\")\n",
        "    \n",
        "    # Show sample comparison\n",
        "    print(f\"\\nSample comparison (first 10 overlapping dates):\")\n",
        "    print(comparison[['DATE', 'station_code', 'pcpn_noaa', 'PRCP', 'diff']].head(10))\n",
        "    \n",
        "else:\n",
        "    print(\"Both dataframes not available for comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis 3: Time Series Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_noaa_a not available\n"
          ]
        }
      ],
      "source": [
        "# Time series analysis - monthly aggregates\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'df_noaa_a' in locals() and df_noaa_a is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"TIME SERIES ANALYSIS - df_noaa_a\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Add month and year columns\n",
        "    df_noaa_a['year'] = df_noaa_a['DATE'].dt.year\n",
        "    df_noaa_a['month'] = df_noaa_a['DATE'].dt.month\n",
        "    \n",
        "    # Monthly aggregates\n",
        "    monthly_a = df_noaa_a.groupby(['year', 'month', 'station_code']).agg({\n",
        "        'pcpn_noaa': 'sum',\n",
        "        'snow_noaa': 'sum',\n",
        "        'snwd_noaa': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"\\nMonthly aggregates calculated:\")\n",
        "    print(f\"  Total monthly records: {len(monthly_a):,}\")\n",
        "    print(f\"\\nSample monthly data:\")\n",
        "    print(monthly_a.head(10))\n",
        "    \n",
        "    # Yearly aggregates\n",
        "    yearly_a = df_noaa_a.groupby(['year', 'station_code']).agg({\n",
        "        'pcpn_noaa': 'sum',\n",
        "        'snow_noaa': 'sum',\n",
        "        'snwd_noaa': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"\\nYearly aggregates:\")\n",
        "    print(yearly_a)\n",
        "    \n",
        "else:\n",
        "    print(\"df_noaa_a not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_noaa_b not available\n"
          ]
        }
      ],
      "source": [
        "# Time series analysis for df_noaa_b\n",
        "if 'df_noaa_b' in locals() and df_noaa_b is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"TIME SERIES ANALYSIS - df_noaa_b\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Add month and year columns\n",
        "    df_noaa_b['year'] = df_noaa_b['DATE'].dt.year\n",
        "    df_noaa_b['month'] = df_noaa_b['DATE'].dt.month\n",
        "    \n",
        "    # Monthly aggregates\n",
        "    monthly_b = df_noaa_b.groupby(['year', 'month', 'station_code']).agg({\n",
        "        'PRCP': 'sum',\n",
        "        'SNOW': 'sum',\n",
        "        'SNWD': 'mean',\n",
        "        'TMAX': 'mean',\n",
        "        'TMIN': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"\\nMonthly aggregates calculated:\")\n",
        "    print(f\"  Total monthly records: {len(monthly_b):,}\")\n",
        "    print(f\"\\nSample monthly data:\")\n",
        "    print(monthly_b.head(10))\n",
        "    \n",
        "    # Yearly aggregates\n",
        "    yearly_b = df_noaa_b.groupby(['year', 'station_code']).agg({\n",
        "        'PRCP': 'sum',\n",
        "        'SNOW': 'sum',\n",
        "        'SNWD': 'mean',\n",
        "        'TMAX': 'mean',\n",
        "        'TMIN': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"\\nYearly aggregates:\")\n",
        "    print(yearly_b)\n",
        "    \n",
        "else:\n",
        "    print(\"df_noaa_b not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis 4: Visualization Ready\n",
        "\n",
        "Data is now ready for plotting. Use the monthly and yearly aggregates created above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openmesh",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
